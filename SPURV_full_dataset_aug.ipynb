{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=5, micro=6, releaselevel='final', serial=0)\n",
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import itertools\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Cropping2D, Dropout, Conv2D, concatenate, TimeDistributed, CuDNNLSTM, LSTM, BatchNormalization\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import mean_squared_error, binary_crossentropy\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# import pydotplus\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlc_one_hot = { 1: [1,0,0,0,0,0], 2:[0,1,0,0,0,0], 3:[0,0,1,0,0,0], 4:[0,0,0,1,0,0], 5:[0,0,0,0,1,0], 6:[0,0,0,0,0,1]}\n",
    "#hlc_one_hot = { 1: [1,0,0,0], 2:[0,1,0,0], 3:[0,0,1,0], 4:[0,0,0,1]}\n",
    "\n",
    "def flatten_list(items_list):\n",
    "    \"\"\" Removes the episode dimension from a list of data points \"\"\"\n",
    "    ret = []\n",
    "    for items in items_list:\n",
    "        for item in items:\n",
    "            ret.append(item)        \n",
    "    return ret      \n",
    "\n",
    "# Custom loss function \n",
    "def weighted_mse(y_true, y_pred, weight_mask):\n",
    "    \"\"\" \n",
    "    Custom loss fucntion, different weigted loss to steer/throttle/brake \n",
    "    Used when all outputs is evaluated by same loss function \n",
    "    \"\"\"\n",
    "    return K.mean(K.square(y_pred - y_true)*weight_mask, axis=-1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\" Custom loss function, RMSE \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "    \n",
    "def steer_loss():\n",
    "    \"\"\" Loss function for steering, RMSE \"\"\"\n",
    "    def custom(y_true, y_pred):\n",
    "        return root_mean_squared_error(y_true, y_pred)\n",
    "    return custom\n",
    "\n",
    "def get_hlc_one_hot(hlc):\n",
    "    \"\"\" One-hot encode HLC values \"\"\"\n",
    "    return hlc_one_hot[hlc]\n",
    "\n",
    "def sine_encode(angle):\n",
    "    \"\"\" Encode steering angle as sine wave \"\"\"\n",
    "    angle_max = 1\n",
    "    N = 10\n",
    "    ret = []\n",
    "    \n",
    "    for i in range(1,N+1,1):\n",
    "        Y = np.sin(((2*np.pi*(i-1))/(N-1))-((angle*np.pi)/(2*angle_max)))\n",
    "        ret.append(Y)\n",
    "    \n",
    "    return np.array(ret)\n",
    "\n",
    "def create_input_dict():\n",
    "    return {\n",
    "        \"forward_imgs\": [],\n",
    "        \"info_signals\": [],\n",
    "        \"hlcs\" : [],\n",
    "        \"environments\": []\n",
    "    }\n",
    "\n",
    "def create_target_dict():\n",
    "    return {\n",
    "        \"steer\" : [],\n",
    "        \"throttle\": [],\n",
    "        \"brake\" : []\n",
    "    }\n",
    "\n",
    "def split_dict(dictionary, split_pos):\n",
    "    \"\"\" Split data into training and validation \"\"\"\n",
    "    train_dict = {}\n",
    "    val_dict = {}\n",
    "    \n",
    "    for key in dictionary:\n",
    "        train_dict[key] = dictionary[key][:split_pos]\n",
    "        val_dict[key] = dictionary[key][split_pos:]\n",
    "    \n",
    "    return train_dict, val_dict\n",
    "\n",
    "def adjust_hlcs(hlcs, info_signals):\n",
    "    \"\"\" \n",
    "    Randomly adjust HLC backwards, \n",
    "    such that the HLC data is given before the car is in the intersection \n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over all episodes \n",
    "    for e in range(len(hlcs)): \n",
    "        active_change_hlc = None\n",
    "        changed = 0\n",
    "        adjust_num = random.randint(20, 40)\n",
    "        # Iterate over all hlcs in episode \n",
    "        for i in range(len(hlcs[e])-2,-1, -1):\n",
    "            next_hlc = np.argmax(hlcs[e][i+1]) + 1 \n",
    "            current_hlc = np.argmax(hlcs[e][i]) +1 \n",
    "            if active_change_hlc == False:\n",
    "                active_change_hlc = None\n",
    "                continue\n",
    "\n",
    "            if active_change_hlc is None:\n",
    "                if next_hlc == 1 or next_hlc == 2 or next_hlc == 3:                     \n",
    "                    if current_hlc == 4: \n",
    "                        active_change_hlc = hlcs[e][i+1]\n",
    "            if active_change_hlc is not None: \n",
    "                hlcs[e][i] = active_change_hlc\n",
    "                if info_signals[e][i][2] != 0: \n",
    "                    changed += 1\n",
    "                if changed == adjust_num:\n",
    "                    active_change_hlc = False\n",
    "                    changed = 0 \n",
    "    return hlcs \n",
    "\n",
    "#### BALANCING DATA - Helper functions #### \n",
    "def get_hlc_dist_label(hlc): \n",
    "    \" Get distributions of HLC in data \"\n",
    "    if hlc == 1: \n",
    "        return \"left\"\n",
    "    elif hlc == 2:\n",
    "        return \"right\"\n",
    "    elif hlc == 3:\n",
    "        return \"straight\"\n",
    "    elif hlc == 4:\n",
    "        return \"follow_lane\"\n",
    "    elif hlc == 5:\n",
    "        return \"change_lane_left\"\n",
    "    elif hlc == 6:\n",
    "        return \"change_lane_right\"\n",
    "    \n",
    "def get_speed_dist_label(speed): \n",
    "    \"\"\" Get distributions of speed in data \"\"\"\n",
    "\n",
    "    if speed < 0.001: \n",
    "        return \"low_speed\"\n",
    "    else: \n",
    "        return \"high_speed\"\n",
    "    \n",
    "def get_speed_limit_dist_label(speed_limit): \n",
    "    \"\"\" Get distributions of speed limits in data \"\"\"\n",
    "\n",
    "    speed_limit = round(speed_limit, 1)\n",
    "    if speed_limit == 0.3: \n",
    "        return \"30km/h\"\n",
    "    elif speed_limit == 0.6: \n",
    "        return \"60km/h\"\n",
    "    elif speed_limit == 0.9: \n",
    "        return \"90km/h\"\n",
    "    \n",
    "def get_percentage_dist(dist): \n",
    "    \"\"\" Get distribution of data in percentage \"\"\"\n",
    "    N = dist[\"traffic_light\"][\"red\"] + dist[\"traffic_light\"][\"green\"]\n",
    "    for dist_key in dist:\n",
    "        for key, dist_val in dist[dist_key].items(): \n",
    "            dist[dist_key][key] = dist_val/N \n",
    "    return dist \n",
    "\n",
    "def get_drop_num(tot_num, num, keep_fraction):\n",
    "    \"\"\" Calculates how many datasamples one should drop to get the right amount of data samples \"\"\"\n",
    "    return int((num - keep_fraction*tot_num)/(1-keep_fraction))\n",
    "\n",
    "\n",
    "def shuffle_data(inputs_flat, targets_flat):\n",
    "    # Shuffle\n",
    "    indices = np.arange(len(inputs_flat[\"forward_imgs\"]))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for key in inputs_flat: \n",
    "        inputs_flat[key] = np.array(inputs_flat[key])[indices]\n",
    "        \n",
    "    for key in targets_flat: \n",
    "        targets_flat[key] = np.array(targets_flat[key])[indices]\n",
    "\n",
    "    return inputs_flat, targets_flat\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations \n",
    "- Lightness\n",
    "- Hue \n",
    "- Gaussian Blur\n",
    "- Shadows \n",
    "- Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(lab_img): \n",
    "    bgr_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2BGR) \n",
    "    rgb_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB) \n",
    "    plt.imshow(rgb_img)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def generate_shadow_coordinates(imshape, no_of_shadows=1):\n",
    "    vertices_list=[]\n",
    "    for index in range(no_of_shadows):\n",
    "        vertex=[]\n",
    "        for dimensions in range(np.random.randint(3,15)): ## Dimensionality of the shadow polygon\n",
    "            vertex.append(( imshape[1]*np.random.uniform(),imshape[0]//3+imshape[0]*np.random.uniform()))\n",
    "        vertices = np.array([vertex], dtype=np.int32) ## single shadow vertices \n",
    "        vertices_list.append(vertices)\n",
    "    return vertices_list ## List of shadow vertices\n",
    "\n",
    "def generate_random_lines(imshape,slant,drop_length):    \n",
    "    drops=[]    \n",
    "    for i in range(1500): \n",
    "        ## If You want heavy rain, try increasing this        \n",
    "        if slant<0:            \n",
    "            x= np.random.randint(slant,imshape[1])        \n",
    "        else:            \n",
    "            x= np.random.randint(0,imshape[1]-slant)        \n",
    "        y= np.random.randint(0,imshape[0]-drop_length)        \n",
    "        \n",
    "        drops.append((x,y))    \n",
    "    return drops \n",
    "\n",
    "def change_light(bgr_img, light_coeff, debug=False):\n",
    "    \n",
    "    lab_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB) \n",
    "    lab_img[:,:,0] = np.minimum(lab_img[:,:,0]*light_coeff, 255)\n",
    "    if debug: \n",
    "        show_image(lab_img)\n",
    "    \n",
    "    return np.array(lab_img, dtype = np.uint8)\n",
    "    \n",
    "def change_hue(bgr_img, hue_coeff, debug=False): \n",
    "    hsv_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2HSV) \n",
    "    hsv_img[:,:,0] = (hsv_img[:,:,0]*hue_coeff)%255   \n",
    "    \n",
    "    bgr_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR) \n",
    "    lab_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    if debug: \n",
    "        show_image(lab_img)\n",
    "    return np.array(lab_img, dtype = np.uint8)\n",
    "           \n",
    "\n",
    "def add_rain(bgr_img, debug=False):     \n",
    "    bgr_img = np.copy(bgr_img)\n",
    "    imshape = bgr_img.shape    \n",
    "    slant_extreme = 10   \n",
    "    slant= 0 #np.random.randint(-slant_extreme,slant_extreme)     \n",
    "    drop_length=7\n",
    "    drop_width=1   \n",
    "    drop_color=(200,200,200) ## a shade of gray   \n",
    "    rain_drops= generate_random_lines(imshape,slant,drop_length)        \n",
    "    for rain_drop in rain_drops:        \n",
    "        cv2.line(bgr_img,(rain_drop[0],rain_drop[1]),(rain_drop[0]+slant,rain_drop[1]+drop_length),drop_color,drop_width)    \n",
    "    bgr_img= cv2.blur(bgr_img,(3,3)) ## rainy view are blurry         \n",
    "    \n",
    "    lab_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    if debug: \n",
    "        show_image(lab_img)\n",
    "    \n",
    "    return np.array(lab_img, dtype = np.uint8)\n",
    "\n",
    "    \n",
    "\n",
    "def add_shadow(bgr_img,no_of_shadows=1, debug=False):\n",
    "    lab_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB) \n",
    "    mask = np.zeros_like(lab_img) \n",
    "    img_shape = lab_img.shape\n",
    "    vertices_list= generate_shadow_coordinates(img_shape, no_of_shadows) #3 getting list of shadow vertices\n",
    "    for vertices in vertices_list: \n",
    "        cv2.fillPoly(mask, vertices, 255) ## adding all shadow polygons on empty mask, single 255 denotes only red channel\n",
    "    \n",
    "    lab_img[:,:,0][mask[:,:,0]==255] = lab_img[:,:,0][mask[:,:,0]==255]*0.2   ## if red channel is hot, image's \"Lightness\" channel's brightness is lowered \n",
    "    \n",
    "    if debug: \n",
    "        show_image(lab_img)\n",
    "    \n",
    "    return np.array(lab_img, dtype = np.uint8)\n",
    "\n",
    "def gaussian_blur(bgr_img, amount=3, debug=False):\n",
    "    bgr_img = cv2.GaussianBlur(bgr_img,(amount,amount),0)\n",
    "    lab_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB)\n",
    "    if debug: \n",
    "        show_image(lab_img)\n",
    "    \n",
    "    return np.array(lab_img, dtype = np.uint8)\n",
    "\n",
    "print_aug = False  \n",
    "\n",
    "if print_aug: \n",
    "    test_path = \"dataset/etron/Town04/ClientAP/cars_no_rain_noise5/2019-06-18_18-23-14/imgs/forward_center_rgb_00000082.png\"\n",
    "    #test_path = \"dataset/etron/Town04/ClientAP/cars_no_rain_noise5/2019-06-18_18-17-14/imgs/forward_right_rgb_00000062.png\"\n",
    "    bgr_img = cv2.imread(test_path)\n",
    "    print(\"Original image\")\n",
    "    show_image(cv2.cvtColor(bgr_img, cv2.COLOR_BGR2LAB))\n",
    "\n",
    "    print(\"Gaussian\")\n",
    "    blur_img = gaussian_blur(bgr_img, amount=11, debug=True)\n",
    "\n",
    "    print(\"Rain\")\n",
    "    rain_img = add_rain(bgr_img, debug=True)\n",
    "\n",
    "    print(\"Shadows\")\n",
    "    shadow_img = add_shadow(bgr_img, debug=True)\n",
    "\n",
    "    print(\"Lightness\")\n",
    "    light_img = change_light(bgr_img, light_range[0], debug=True)\n",
    "\n",
    "    print(\"Hue\")\n",
    "    hue_img = change_hue(bgr_img, hue_range[1], debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load driving logs\n",
    "- Loads data from all csv file in a lits of folders \n",
    "- Stores the data in a input dictionary and a target dictionary \n",
    "- Normalizes and coverts data to correct format \n",
    "- Does not use side cameraes for lane change data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_driving_logs(dataset_folders, steering_correction):\n",
    "    \"\"\" \n",
    "    input: \n",
    "        dataset_folders: list of paths to folders to load data from \n",
    "        steering_correction: float, adjusts steering angles of forward facing side cameras \n",
    "        \n",
    "    Loads data from all csv file in a lits of folders \n",
    "    Stores the data in a input dictionary and a target dictionary \n",
    "    Normalizes and coverts data to correct format \n",
    "    Does not use side cameraes for lane change data\n",
    "    \n",
    "    return: \n",
    "        inputs: dictionary of all input data (image paths, info signals, HLCs, control signals)\n",
    "        targets: dictionary of all target data (steer, throttle, brake)\n",
    "    \"\"\"\n",
    "    img_paths_center = []\n",
    "    \n",
    "    \n",
    "    inputs = create_input_dict()\n",
    "    \n",
    "    targets = create_target_dict()\n",
    "    # Loads data \n",
    "    for folder in dataset_folders:\n",
    "        print(\"Checking folder: {:s}...\".format(folder))\n",
    "        folder_path = Path(\"dataset\") / folder\n",
    "        for episode in glob(str(folder_path / \"*\")):\n",
    "            \n",
    "            \n",
    "            temp_forward = {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_info_signals =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_hlcs =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_environments =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "\n",
    "            temp_steer =  {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_throttle = {\"center\": [], \"left\": [], \"right\": []}\n",
    "            temp_brake = {\"center\": [], \"left\": [], \"right\": []}\n",
    "                \n",
    "            episode_path = Path(episode)\n",
    "\n",
    "            df = pd.read_csv(str(episode_path / \"driving_log.csv\"))\n",
    "            for index, row in df.iterrows():\n",
    "                if index == 0: \n",
    "                    continue \n",
    "                    \n",
    "                speed = 0#float(row[\"Velocity\"]) * 3.6 / 100\n",
    "                speed_limit = 0#float(row[\"SpeedLimit\"]) * 3.6 / 100\n",
    "                traffic_light = 0#int(row[\"TrafficLight\"])\n",
    "                environment = 0#int(row[\"Environment\"])\n",
    "                \n",
    "                #controls = literal_eval(row[\"Controls\"])\n",
    "                    \n",
    "                steer = row[\"angle\"]\n",
    "                throttle = row[\"speed\"]\n",
    "                brake = 0#controls[2]\n",
    "                \n",
    "                \"\"\"hlc = get_hlc_one_hot(row[\"HLC\"])\"\"\"\n",
    "                \n",
    "                hlc = row[\"high_level_command\"]\n",
    "                if hlc == 0 or hlc == -1:\n",
    "                    hlc = 4\n",
    "                hlc = get_hlc_one_hot(hlc)\n",
    "                \n",
    "                temp_forward[\"center\"].append(str(episode_path / row[\"image_path\"]))              \n",
    "                temp_steer[\"center\"].append(steer)\n",
    "                temp_throttle[\"center\"].append(throttle)\n",
    "                temp_brake[\"center\"].append(brake)\n",
    "                temp_info_signals[\"center\"].append([speed, speed_limit, traffic_light])\n",
    "                temp_hlcs[\"center\"].append(hlc)\n",
    "                temp_environments[\"center\"].append(environment)\n",
    "                \n",
    "                # Only use the left/right shifted images for non lange-change data\n",
    "                #if row[\"HLC\"] != 5 and row[\"HLC\"] != 6:\n",
    "                #temp_forward[\"left\"].append(str(episode_path / row[\"ForwardLeft\"]))\n",
    "                #temp_forward[\"right\"].append(str(episode_path / row[\"ForwardRight\"])) \n",
    "                \n",
    "                temp_forward[\"left\"].append(str(episode_path / row[\"image_path\"]))\n",
    "                temp_forward[\"right\"].append(str(episode_path / row[\"image_path\"])) \n",
    "                \n",
    "                temp_throttle[\"left\"].append(throttle)\n",
    "                temp_throttle[\"right\"].append(throttle)\n",
    "\n",
    "                temp_steer[\"left\"].append(steer+steering_correction)\n",
    "                temp_steer[\"right\"].append(steer-steering_correction)\n",
    "\n",
    "                temp_brake[\"left\"].append(brake)\n",
    "                temp_brake[\"right\"].append(brake)\n",
    "\n",
    "                temp_info_signals[\"left\"].append([speed, speed_limit, traffic_light])\n",
    "                temp_info_signals[\"right\"].append([speed, speed_limit, traffic_light])\n",
    "\n",
    "                temp_hlcs[\"left\"].append(hlc)\n",
    "                temp_hlcs[\"right\"].append(hlc)\n",
    "                \n",
    "                temp_environments[\"left\"].append(environment)\n",
    "                temp_environments[\"right\"].append(environment)\n",
    "\n",
    "                    \n",
    "            inputs[\"forward_imgs\"].append(temp_forward[\"center\"])\n",
    "            #inputs[\"forward_imgs\"].append(temp_forward[\"left\"])\n",
    "            #inputs[\"forward_imgs\"].append(temp_forward[\"right\"])\n",
    "            \n",
    "            inputs[\"info_signals\"].append(temp_info_signals[\"center\"])\n",
    "            #inputs[\"info_signals\"].append(temp_info_signals[\"left\"])\n",
    "            #inputs[\"info_signals\"].append(temp_info_signals[\"right\"])\n",
    "            \n",
    "            inputs[\"hlcs\"].append(temp_hlcs[\"center\"])\n",
    "            #inputs[\"hlcs\"].append(temp_hlcs[\"left\"])\n",
    "            #inputs[\"hlcs\"].append(temp_hlcs[\"right\"])\n",
    "            \n",
    "            inputs[\"environments\"].append(temp_environments[\"center\"])\n",
    "            #inputs[\"environments\"].append(temp_environments[\"left\"])\n",
    "            #inputs[\"environments\"].append(temp_environments[\"right\"])\n",
    "            \n",
    "            targets[\"steer\"].append(temp_steer[\"center\"])\n",
    "            #targets[\"steer\"].append(temp_steer[\"left\"])\n",
    "            #targets[\"steer\"].append(temp_steer[\"right\"])\n",
    "            \n",
    "            targets[\"throttle\"].append(temp_throttle[\"center\"])\n",
    "            #targets[\"throttle\"].append(temp_throttle[\"left\"])\n",
    "            #targets[\"throttle\"].append(temp_throttle[\"right\"])\n",
    "             \n",
    "            targets[\"brake\"].append(temp_brake[\"center\"])\n",
    "            #targets[\"brake\"].append(temp_brake[\"left\"])\n",
    "            #targets[\"brake\"].append(temp_brake[\"right\"])\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    print(\"Done, {:d} episode(s) loaded.\".format(len(inputs[\"forward_imgs\"])))\n",
    "\n",
    "    return(inputs, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(dist, title=\"\"):\n",
    "    \"\"\" Plots distribution of HLC, speed and traffic lights \"\"\"\n",
    "    print(\"Plotting...\")\n",
    "    tot_num = 0\n",
    "    for key in dist[\"environment\"]:\n",
    "        tot_num += dist[\"environment\"][key]\n",
    "        \n",
    "    fig = plt.figure(figsize=(16,6))\n",
    "\n",
    "    # HLC\n",
    "    labels =[\"Follow Lane\", \"Left\", \"Right\", \"Straight\", \"Change Lane Left\", \"Change Lane Right\"]\n",
    "    sizes = [dist[\"hlc\"][\"follow_lane\"], dist[\"hlc\"][\"left\"], dist[\"hlc\"][\"right\"], dist[\"hlc\"][\"straight\"], dist[\"hlc\"][\"change_lane_left\"], dist[\"hlc\"][\"change_lane_right\"], ]\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,4,1)\n",
    "    wedges, texts, autotexts = ax1.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "    ax1.set_title(\"Distrubution of HLC\")\n",
    "    ax1.legend(wedges,labels, loc=\"best\")\n",
    "    ax1.axis('equal')\n",
    "    \n",
    "\n",
    "    # Speed\n",
    "    labels =[\"Low Speed\", \"High Speed\"]\n",
    "    sizes = [dist[\"speed\"][\"low_speed\"], dist[\"speed\"][\"high_speed\"]]\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,4,2)\n",
    "    wedges, texts, autotexts = ax2.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax2.set_title(\"Distrubution of speed\")\n",
    "    ax2.legend(wedges,labels, loc=\"best\")\n",
    "    ax2.axis('equal')\n",
    "\n",
    "    # Traffic Light\n",
    "    labels =[\"Red\", \"Green\"]\n",
    "    sizes = [dist[\"traffic_light\"][\"red\"], dist[\"traffic_light\"][\"green\"]]\n",
    "    \n",
    "    ax3 = fig.add_subplot(1,4,3)\n",
    "    wedges, texts, autotexts = ax3.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax3.set_title(\"Distrubution of traffic light\")\n",
    "    ax3.legend(wedges,labels, loc=\"best\")\n",
    "    ax3.axis('equal')\n",
    "    \n",
    "    # Environment\n",
    "    labels =[\"Town 1\", \"Town 4\"]\n",
    "    sizes = [dist[\"environment\"][\"town1\"], dist[\"environment\"][\"town4\"]]\n",
    "    \n",
    "    ax4 = fig.add_subplot(1,4,4)\n",
    "    wedges, texts, autotexts = ax4.pie(sizes, autopct='%.1f%%')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "    ax4.set_title(\"Distrubution of environments\")\n",
    "    ax4.legend(wedges,labels, loc=\"best\")\n",
    "    ax4.axis('equal')\n",
    "    \n",
    "    fig.suptitle(title + \": \" + str(tot_num) + \" sequences\", fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Data \n",
    "\n",
    "- Get distribution of HLC, speed, speed limit, and traffic lights \n",
    "- Balance data for LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(inputs): \n",
    "    \"\"\" \n",
    "        input:\n",
    "            inputs: dictionary of input data \n",
    "                \n",
    "        return: \n",
    "            dist: dictionary of distributions for HLC, speed, speed limit, and traffic lights \n",
    "    \"\"\"\n",
    "    dist = {\n",
    "        \"hlc\": {\n",
    "            \"left\": 0,\n",
    "            \"right\": 0, \n",
    "            \"straight\": 0,\n",
    "            \"follow_lane\": 0,\n",
    "            \"change_lane_left\": 0, \n",
    "            \"change_lane_right\": 0\n",
    "        },\n",
    "        \"environment\": {\n",
    "            \"town1\": 0,\n",
    "            \"town4\": 0\n",
    "        },\n",
    "        \"speed\": {\n",
    "            \"low_speed\": 0, \n",
    "            \"high_speed\": 0,\n",
    "        }, \n",
    "        \"speed_limit\": {\n",
    "            \"30km/h\": 0,\n",
    "            \"60km/h\": 0,\n",
    "            \"90km/h\": 0\n",
    "        }, \n",
    "        \"traffic_light\": {\n",
    "            \"red\": 0, \n",
    "            \"green\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # HLC distribution\n",
    "    for hlcs in inputs[\"hlcs\"]: \n",
    "        \n",
    "        follow_lane = True \n",
    "        # Iterate over all HLC in sequence \n",
    "        for hlc in hlcs: \n",
    "            hlc_value = np.argmax(hlc) + 1 \n",
    "            # Only mark sequence as follow lane if all hlcs are follow lane  \n",
    "            if hlc_value != 4:\n",
    "                follow_lane = False \n",
    "                hlc_label = get_hlc_dist_label(hlc_value)\n",
    "                dist[\"hlc\"][hlc_label] += 1 \n",
    "                break\n",
    "        if follow_lane: \n",
    "            dist[\"hlc\"][\"follow_lane\"] += 1  \n",
    "            \n",
    "    # Environment distribution \n",
    "    for environment in inputs[\"environments\"]: \n",
    "        if environment[0] == 0: \n",
    "            dist[\"environment\"][\"town4\"] +=1 \n",
    "        if environment[0] == 1: \n",
    "            dist[\"environment\"][\"town1\"] +=1\n",
    "            \n",
    "    # Info signals distribution \n",
    "    for info_signals in inputs[\"info_signals\"]: \n",
    "        red_light = True \n",
    "        speeds = []\n",
    "        speed_limits = []\n",
    "        \n",
    "        # For each sequence \n",
    "        for info_signal in info_signals: \n",
    "            speeds.append(info_signal[0])                \n",
    "            speed_limits.append(info_signal[1]*10)\n",
    "            \n",
    "            traffic_light = info_signal[2]\n",
    "            if traffic_light == 1: \n",
    "                red_light = False \n",
    "                \n",
    "                \n",
    "        # Update dist after each sequence \n",
    "        \n",
    "        # Max speed \n",
    "        speed_label = get_speed_dist_label(np.max(np.array(speeds)))\n",
    "        dist[\"speed\"][speed_label] += 1 \n",
    "\n",
    "        # Most frequent speed limit \n",
    "        most_frequent_speed_limit = np.argmax(np.bincount(speed_limits))/10\n",
    "        speed_limit_label = get_speed_limit_dist_label(most_frequent_speed_limit) \n",
    "        dist[\"speed_limit\"][speed_limit_label] += 1 \n",
    "        \n",
    "        # Mark sequemce as green light if it has any frame with green light \n",
    "        if red_light: \n",
    "            dist[\"traffic_light\"][\"red\"] += 1\n",
    "        else: \n",
    "            dist[\"traffic_light\"][\"green\"] += 1\n",
    "        \n",
    "        \n",
    "    return dist   \n",
    "    \n",
    "\n",
    "\n",
    "def balance_speed(inputs, targets, dist, target_key, target_fraction):\n",
    "    \n",
    "    \"\"\" Balance speed such that target fraction is correct \"\"\"\n",
    "    \n",
    "    # Calculate number of data samples that should be droped \n",
    "    tot_num = dist[\"speed\"][\"low_speed\"] + dist[\"speed\"][\"high_speed\"]\n",
    "    target_speed_num =  dist[\"speed\"][target_key] \n",
    "    drop_target_speed_num = get_drop_num(tot_num, target_speed_num, target_fraction)\n",
    "\n",
    "    inputs_bal = create_input_dict()\n",
    "    targets_bal = create_target_dict()\n",
    "    \n",
    "    # Balance low speed \n",
    "    dropped = 0\n",
    "    for i in range(len(inputs[\"info_signals\"])): \n",
    "        info_signals = inputs[\"info_signals\"][i]\n",
    "        speeds = []\n",
    "        for info_signal in info_signals:\n",
    "            speeds.append(info_signal[0])\n",
    "        speed = np.max(speeds)\n",
    "            \n",
    "        # Drop        \n",
    "        if get_speed_dist_label(speed) == target_key and (drop_target_speed_num>dropped):\n",
    "            dropped += 1 \n",
    "            continue\n",
    "        \n",
    "        # Keep \n",
    "        for key in inputs_bal: \n",
    "            inputs_bal[key].append(inputs[key][i])\n",
    "        for key in targets_bal: \n",
    "            targets_bal[key].append(targets[key][i])\n",
    "        \n",
    "    return inputs_bal, targets_bal \n",
    "    \n",
    "def balance_traffic_light(inputs, targets, dist, target_key, target_fraction):\n",
    "    \"\"\" Balance traffic lights such that target fraction is correct \"\"\"\n",
    "    \n",
    "    # Calculate number of data samples that should be droped \n",
    "    tot_num = dist[\"traffic_light\"][\"red\"] + dist[\"traffic_light\"][\"green\"]\n",
    "    target_speed_num =  dist[\"traffic_light\"][target_key] \n",
    "    drop_target_traffic_light_num = get_drop_num(tot_num, target_speed_num, target_fraction)\n",
    "\n",
    "    inputs_bal = create_input_dict()\n",
    "    targets_bal = create_target_dict() \n",
    "\n",
    "    # Balance traffic lights \n",
    "    dropped = 0\n",
    "    for i in range(len(inputs[\"info_signals\"])): \n",
    "        info_signals = inputs[\"info_signals\"][i]\n",
    "        \n",
    "        # Mark episode as green light if there is at least one green traffic light in sequence \n",
    "        traffic_lights = []\n",
    "        for info_signal in info_signals:\n",
    "            traffic_light = \"red\" if info_signal[2] == 0 else \"green\"\n",
    "            traffic_lights.append(traffic_light)\n",
    "        traffic_light = \"green\" if \"green\" in traffic_lights else \"red\" \n",
    "            \n",
    "        # Drop        \n",
    "        if traffic_light == target_key and (drop_target_traffic_light_num>dropped):\n",
    "            dropped += 1\n",
    "            continue\n",
    "        \n",
    "        # Keep \n",
    "        for key in inputs_bal: \n",
    "            inputs_bal[key].append(inputs[key][i])\n",
    "        for key in targets_bal: \n",
    "            targets_bal[key].append(targets[key][i])     \n",
    "\n",
    "    return inputs_bal, targets_bal \n",
    "\n",
    "def balance_hlc(inputs, targets, dist, lane_change):\n",
    "    \"\"\" Balance HLC (except follow lane) such that target fraction is correct \"\"\"\n",
    "\n",
    "    # Get correct dist data TODO: combine to one dict ? \n",
    "    if lane_change: \n",
    "        num_hlcs = {\n",
    "            \"change_lane_left\": dist[\"hlc\"][\"change_lane_left\"], \n",
    "            \"change_lane_right\": dist[\"hlc\"][\"change_lane_right\"]\n",
    "        }\n",
    "    else: \n",
    "        num_hlcs = {\n",
    "            \"straight\": dist[\"hlc\"][\"straight\"],\n",
    "            \"left\": dist[\"hlc\"][\"left\"],\n",
    "            \"right\": dist[\"hlc\"][\"right\"]\n",
    "        }\n",
    "    freqs = []\n",
    "    for key in num_hlcs : \n",
    "        freqs.append(num_hlcs[key])\n",
    "    num = np.min(freqs)\n",
    "    \n",
    "    inputs_bal = create_input_dict()\n",
    "    targets_bal = create_target_dict() \n",
    "    \n",
    "    for i in range(len(inputs[\"hlcs\"])): \n",
    "        # Find current hlc \n",
    "        hlcs = inputs[\"hlcs\"][i]\n",
    "        current_hlc = \"follow_lane\"\n",
    "        for hlc in hlcs: \n",
    "            hlc_type = get_hlc_dist_label(np.argmax(hlc)+1)\n",
    "            if hlc_type != \"follow_lane\": \n",
    "                current_hlc = hlc_type\n",
    "                break\n",
    "        # Drop        \n",
    "        if current_hlc != \"follow_lane\" and (num_hlcs[current_hlc]>num):\n",
    "            num_hlcs[current_hlc]-=1\n",
    "            continue\n",
    "        \n",
    "        # Keep \n",
    "        for key in inputs_bal: \n",
    "            inputs_bal[key].append(inputs[key][i])\n",
    "        for key in targets_bal: \n",
    "            targets_bal[key].append(targets[key][i])\n",
    "\n",
    "    return inputs_bal, targets_bal \n",
    "        \n",
    "\n",
    "def balance_follow_lane(inputs, targets, dist, target_fraction):\n",
    "    \"\"\" Balance follow lane such that target fraction is correct \"\"\"\n",
    "    \n",
    "    tot_num = 0 \n",
    "    target_hlc_num = 0\n",
    "    for hlc_key in dist[\"hlc\"]:\n",
    "        tot_num += dist[\"hlc\"][hlc_key] \n",
    "            \n",
    "        \n",
    "    target_hlc_num =  dist[\"hlc\"][\"follow_lane\"] \n",
    "    drop_target_hlc_num = get_drop_num(tot_num, target_hlc_num, target_fraction)\n",
    "\n",
    "    inputs_bal = create_input_dict()\n",
    "    targets_bal = create_target_dict() \n",
    "    \n",
    "    dropped = 0\n",
    "    \n",
    "    # Find current hlc \n",
    "    for i in range(len(inputs[\"hlcs\"])): \n",
    "        hlcs = inputs[\"hlcs\"][i]\n",
    "        current_hlc = \"follow_lane\"\n",
    "        for hlc in hlcs: \n",
    "            hlc_type = get_hlc_dist_label(np.argmax(hlc)+1)\n",
    "            if hlc_type != \"follow_lane\": \n",
    "                current_hlc = hlc_type\n",
    "                break\n",
    "            \n",
    "        # Drop        \n",
    "        if current_hlc == \"follow_lane\" and (drop_target_hlc_num>dropped):\n",
    "            dropped +=1\n",
    "            continue\n",
    "        \n",
    "        # Keep \n",
    "        for key in inputs_bal: \n",
    "            inputs_bal[key].append(inputs[key][i])\n",
    "        for key in targets_bal: \n",
    "            targets_bal[key].append(targets[key][i])\n",
    "\n",
    "    return inputs_bal, targets_bal \n",
    "    \n",
    "\n",
    "def balance_data_lstm(inputs, targets, follow_lane_frac=0.7, red_light_frac=0.01, low_speed_frac=0.05,debug=False, drop=True):\n",
    "    \"\"\" Balance dataset for LSTM data \"\"\"\n",
    "    print(\"Balancing data:\")\n",
    "    \n",
    "    # Get distribution \n",
    "    dist = get_dist(inputs)\n",
    "    \n",
    "    inputs_bal = inputs\n",
    "    targets_bal = targets\n",
    "    dist_bal = dist \n",
    "    \n",
    "    # Balance traffic lights \n",
    "    print(\"   - Balancing traffic lights\")\n",
    "    inputs_bal, targets_bal = balance_traffic_light(inputs_bal, targets_bal, dist_bal,\"red\", red_light_frac)\n",
    "    dist_bal = get_dist(inputs_bal)\n",
    "    \n",
    "    # Balance low speed \n",
    "    print(\"   - Balancing low speed\")\n",
    "    inputs_bal, targets_bal = balance_speed(inputs_bal, targets_bal, dist_bal,\"low_speed\", low_speed_frac)\n",
    "    \"\"\"\n",
    "    dist_bal = get_dist(inputs_bal)\n",
    "    \n",
    "    # Ballance hlc - left, left, straight \n",
    "    print(\"   - Balancing HLCs\")\n",
    "    inputs_bal, targets_bal = balance_hlc(inputs_bal, targets_bal, dist_bal,False)    \n",
    "    dist_bal = get_dist(inputs_bal)\n",
    "    \n",
    "    # Ballance hlc - follow lane \n",
    "    print(\"   - Balancing LANEFOLLOW\")\n",
    "    inputs_bal, targets_bal = balance_follow_lane(inputs_bal, targets_bal, dist_bal, follow_lane_frac)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Shuffle\n",
    "    inputs_bal, targets_bal = shuffle_data(inputs_bal, targets_bal)\n",
    "    \n",
    "    if drop: \n",
    "        inputs_bal, targets_bal = drop_follow_lane(inputs_bal,targets_bal, 3)\n",
    "    dist_bal = get_dist(inputs_bal)\n",
    "        \n",
    "    return inputs_bal, targets_bal, dist_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop random data \n",
    "Randomly drop data that is follow lane without brake values or turns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_follow_lane(inputs, targets, keep_every): \n",
    "    print(\"Dropping...\")\n",
    "    \n",
    "    inputs_dropped = create_input_dict()\n",
    "    targets_dropped = create_target_dict() \n",
    "    \n",
    "\n",
    "    for i in range(len(inputs[\"forward_imgs\"])): \n",
    "        \n",
    "        \n",
    "        follow_lane = True \n",
    "        # Iterate over all HLC in sequence \n",
    "        for hlc in inputs[\"hlcs\"][i]: \n",
    "            hlc_value = np.argmax(hlc) + 1 \n",
    "            # Only mark sequence as follow lane if all hlcs are follow lane  \n",
    "            if hlc_value != 4:\n",
    "                follow_lane = False \n",
    "                break\n",
    "                \n",
    "        brake = targets[\"brake\"][i]\n",
    "        steer = targets[\"steer\"][i]\n",
    "        \n",
    "        # Only drop if FOLLOW_LANE, no brake, and no turn\n",
    "        if follow_lane and brake == 0 and abs(steer)<0.1:  \n",
    "            # Drop based on drop_rate \n",
    "            \n",
    "            if np.random.randint(keep_every) != 0: \n",
    "                continue\n",
    "\n",
    "        # Keep \n",
    "        for key in inputs_dropped: \n",
    "            inputs_dropped[key].append(inputs[key][i])\n",
    "        for key in targets_dropped: \n",
    "            targets_dropped[key].append(targets[key][i])\n",
    "\n",
    "    return inputs_dropped, targets_dropped \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_sequences(data, sampling_interval, seq_length):\n",
    "    sequences = []\n",
    "    slices = []\n",
    "    for o in range(sampling_interval+1):\n",
    "        slices.append(data[o::sampling_interval+1])\n",
    "    for s in slices:\n",
    "        for o in range(0,len(s)):\n",
    "            if o + seq_length <= len(s):\n",
    "                sequences.append(s[o:o+seq_length])\n",
    "    return sequences\n",
    "\n",
    "def prepare_dataset_lstm(inputs, targets, sampling_interval, seq_length):\n",
    "    \n",
    "    inputs_flat = create_input_dict()\n",
    "    targets_flat = create_target_dict()\n",
    "    \n",
    "    for e in range(len(inputs[\"forward_imgs\"])):\n",
    "        [inputs_flat[\"forward_imgs\"].append(sequence) for sequence in get_episode_sequences(inputs[\"forward_imgs\"][e],sampling_interval, seq_length)]\n",
    "        [inputs_flat[\"info_signals\"].append(sequence) for sequence in get_episode_sequences(inputs[\"info_signals\"][e],sampling_interval, seq_length)]\n",
    "        [inputs_flat[\"hlcs\"].append(sequence) for sequence in get_episode_sequences(inputs[\"hlcs\"][e],sampling_interval, seq_length)]\n",
    "        [inputs_flat[\"environments\"].append(sequence) for sequence in get_episode_sequences(inputs[\"environments\"][e],sampling_interval, seq_length)]\n",
    "        [targets_flat[\"steer\"].append(sequence[-1]) for sequence in get_episode_sequences(targets[\"steer\"][e],sampling_interval, seq_length)]\n",
    "        [targets_flat[\"throttle\"].append(sequence[-1]) for sequence in get_episode_sequences(targets[\"throttle\"][e],sampling_interval, seq_length)]\n",
    "        [targets_flat[\"brake\"].append(sequence[-1]) for sequence in get_episode_sequences(targets[\"brake\"][e],sampling_interval, seq_length)]\n",
    "\n",
    "    \n",
    "    # Shuffle\n",
    "    inputs_flat, targets_flat = shuffle_data(inputs_flat, targets_flat)\n",
    "    \n",
    "    return(inputs_flat, targets_flat)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(seq_length, print_summary=True):\n",
    "    forward_image_input = Input(shape=(seq_length, 162, 217, 3), name=\"forward_image_input\")\n",
    "    info_input = Input(shape=(seq_length,3), name=\"info_input\")\n",
    "    hlc_input = Input(shape=(seq_length,6), name=\"hlc_input\")\n",
    "\n",
    "    x = TimeDistributed(Cropping2D(cropping=((50,0),(0,0))))(forward_image_input)\n",
    "    x = TimeDistributed(Lambda(lambda x: ((x/255.0) - 0.5)))(x)\n",
    "    x = TimeDistributed(Conv2D(24,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(36,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(48,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    conv_output = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x = concatenate([conv_output, info_input, hlc_input])\n",
    "    \n",
    "    x = TimeDistributed(Dense(100, activation=\"relu\"))(x)\n",
    "    x = CuDNNLSTM(10, return_sequences = False)(x)\n",
    "    steer_pred = Dense(10, activation=\"tanh\", name=\"steer_pred\")(x)\n",
    "    \n",
    "    x = TimeDistributed(Cropping2D(cropping=((50,0),(0,0))))(forward_image_input)\n",
    "    x = TimeDistributed(Lambda(lambda x: ((x/255.0) - 0.5)))(x)\n",
    "    x = TimeDistributed(Conv2D(24,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(36,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(48,(5,5),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3),strides=(2,2), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Conv2D(64,(3,3), activation=\"relu\"))(x)\n",
    "    conv_output = TimeDistributed(Flatten())(x)\n",
    "\n",
    "    x = concatenate([conv_output, info_input, hlc_input])\n",
    "    \n",
    "    x = TimeDistributed(Dense(100, activation=\"relu\"))(x)\n",
    "    x = CuDNNLSTM(10, return_sequences = False)(x)\n",
    "    throtte_pred = Dense(1, name=\"throttle_pred\")(x)\n",
    "    brake_pred = Dense(1, name=\"brake_pred\")(x)\n",
    "    \n",
    "    model = Model(inputs=[forward_image_input, info_input, hlc_input], outputs=[steer_pred,throtte_pred,brake_pred])\n",
    "\n",
    "    if print_summary: \n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(image_path):\n",
    "    return \"./dataset/2019-10-09T11:07:52/episode1/images/\" + image_path.split(\"/\")[-1]\n",
    "\n",
    "print(get_path(\"/home/kia/data_dump/2019-10-09T11:07:52/images/55972.jpg\"))\n",
    "class generator(Sequence):\n",
    "    def __init__(self, inputs, targets, batch_size, validation=False):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.validation = validation \n",
    "        \n",
    "        random.seed() \n",
    "        # Convert to np array\n",
    "        for key in inputs: \n",
    "            inputs[key] = np.array(self.inputs[key])\n",
    "\n",
    "        for key in targets: \n",
    "            targets[key] = np.array(self.targets[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.inputs[\"forward_imgs\"]) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subset = np.arange(idx * self.batch_size,min((idx + 1) * self.batch_size,len(self.inputs[\"forward_imgs\"])))\n",
    "        forward_imgs = []\n",
    "        steer_pred = np.array([sine_encode(steer) for steer in self.targets[\"steer\"][subset]])\n",
    "        read_images = [[cv2.resize(cv2.imgread(get_path(path)), fx=0.5, fy=0.5) for path in seq] for seq in self.inputs[\"forward_imgs\"][subset]]\n",
    "        if not self.validation: \n",
    "            augment = 2 #random.randint(0,3)\n",
    "            if augment == 0: \n",
    "                augment_type = random.randint(0,4)\n",
    "                \n",
    "                # Lightness \n",
    "                if augment_type == 0: \n",
    "                    light_coeff = random.random()*0.9+0.5\n",
    "                    for seq in read_images:\n",
    "                        forward_imgs.append([change_light(image), light_coeff=light_coeff) for image in seq])\n",
    "                # Hue \n",
    "                elif augment_type == 1:\n",
    "                    hue_coeff = random.random()*1.4 + 0.2\n",
    "                    for seq in read_images:\n",
    "                        forward_imgs.append([change_hue(image), hue_coeff=hue_coeff) for image in seq])\n",
    "                # Rain \n",
    "                elif augment_type == 2:\n",
    "                    for seq in read_images:\n",
    "                        forward_imgs.append([add_rain(image) for image in seq])\n",
    "                \n",
    "                # Shadows \n",
    "                elif augment_type == 3:\n",
    "                    for seq in read_images:\n",
    "                        forward_imgs.append([add_shadow(image) for image in seq])\n",
    "                # Gaussian blur \n",
    "                elif augment_type == 4:\n",
    "                    for seq in read_images:\n",
    "                        forward_imgs.append([gaussian_blur(image) for image in seq])\n",
    "            else: \n",
    "                for seq in read_images:\n",
    "                    forward_imgs.append([cv2.cvtColor(image,cv2.COLOR_BGR2LAB) for image in seq])\n",
    "        else:\n",
    "            for seq in read_images:\n",
    "                forward_imgs.append([cv2.cvtColor(image,cv2.COLOR_BGR2LAB) for image in seq])\n",
    "\n",
    "        return {\n",
    "            \"forward_image_input\": np.array(forward_imgs), \n",
    "            \"info_input\": self.inputs[\"info_signals\"][subset],\n",
    "            \"hlc_input\": self.inputs[\"hlcs\"][subset]\n",
    "        }, {\n",
    "            \"steer_pred\": steer_pred,\n",
    "            \"throttle_pred\": self.targets[\"throttle\"][subset],\n",
    "            \"brake_pred\": self.targets[\"brake\"][subset]\n",
    "        }\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"test4_new_autopilot\"\n",
    "val_split = 0.8\n",
    "adjust_hlc = False \n",
    "balance_data = False \n",
    "\n",
    "\n",
    "epochs_list = [15]\n",
    "dataset_folders_lists = [[\"2019-10-09T11:07:52\"]]\n",
    "\"\"\"dataset_folders_lists = [([\n",
    "    \"etron/Town01/ClientAP/no_cars_no_rain\",\n",
    "    \"etron/Town01/ClientAP/no_cars_no_rain_noise15\",\n",
    "    \"etron/Town01/ClientAP/no_cars_rain_noise10\",\n",
    "    \"etron/Town01/ClientAP/cars_no_rain_noise15\",\n",
    "    \"etron/Town01/ClientAP/cars_rain_noise10\",    \n",
    "], [\n",
    "    \"etron/Town04/ClientAP/no_cars_no_rain\", \n",
    "    \"etron/Town04/ClientAP/no_cars_rain_noise5\", \n",
    "    \"etron/Town04/ClientAP/cars_no_rain_noise5\", \n",
    "    \"etron/Town04/ClientAP/cars_rain_noise5\", \n",
    "    \"etron/Town01/ClientAP/brake_all_weather_noise15\",\n",
    "    \"etron/Town01/ClientAP/brake_all_weather\",\n",
    "])]\"\"\"\n",
    "\n",
    "\n",
    "steering_corrections = [0.05]\n",
    "\n",
    "batch_sizes = [32]\n",
    "\n",
    "sampling_interval = [2]\n",
    "\n",
    "seq_length = [10]\n",
    "\n",
    "parameter_permutations = itertools.product(epochs_list, \n",
    "                                           dataset_folders_lists, \n",
    "                                           steering_corrections, \n",
    "                                           batch_sizes,\n",
    "                                           sampling_interval,\n",
    "                                           seq_length)  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of model test: initial_spurv_model\n",
      "Balance data y/[n]: n\n",
      "\n",
      "Checking folder: 2019-10-09T11:07:52...\n",
      "Done, 1 episode(s) loaded.\n",
      "Initiate training loop with the following parameters:\n",
      "---\n",
      "epochs:\t\t\t15\n",
      "dataset folders:\t['2019-10-09T11:07:52']\n",
      "steering correction:\t0.05\n",
      "batch size:\t\t32\n",
      "sampling interval:\t2\n",
      "seq lenght: \t\t10\n",
      "\n",
      "\n",
      "---\n",
      "Training set size: 65455\n",
      "Validation set size: 16364\n",
      "Epoch 1/15\n",
      "1464/2045 [====================>.........] - ETA: 27:16 - loss: 0.1886 - steer_pred_loss: 0.1868 - throttle_pred_loss: 0.0017 - brake_pred_loss: 9.2069e-05"
     ]
    }
   ],
   "source": [
    "# Train a new model for each parameter permutation, and save the best models\n",
    "model_name = input(\"Name of model test: \").strip()\n",
    "balance_data = True if input(\"Balance data y/[n]: \").lower() == \"y\" else False \n",
    "#drop_data = True if input(\"Drop data y/[n]: \").lower() == \"y\" else False \n",
    "print(\"\")\n",
    "\n",
    "for parameters in parameter_permutations:\n",
    "    \n",
    "    # Get parameters\n",
    "    epochs, dataset_folders, steering_correction, batch_size, sampling_interval, seq_length = parameters\n",
    "    parameters_string = (\"epochs:\\t\\t\\t{}\\ndataset folders:\\t{}\\nsteering correction:\\t{}\\nbatch size:\\t\\t{}\\nsampling interval:\\t{}\\nseq lenght: \\t\\t{}\\n\\n\"\n",
    "                         .format(epochs, str(dataset_folders), steering_correction, batch_size, sampling_interval, seq_length))\n",
    "    \n",
    "    \n",
    "    #town1_dataset_folders, town4_dataset_folders = dataset_folders\n",
    "\n",
    "    # Prepare for logging\n",
    "    timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.localtime(time.time()))\n",
    "    path = Path('models') / model_name / timestamp\n",
    "    if not os.path.exists(str(path)):\n",
    "        os.makedirs(str(path))\n",
    "        \n",
    "    # Save parmeters to disk\n",
    "    with open(str(path / \"parameters.txt\"), \"w\") as text_file:\n",
    "        text_file.write(parameters_string)\n",
    "    \n",
    "    # Save config file to disk \n",
    "    model_name = \"lstm\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config[\"ModelConfig\"] = {'Model': model_name,'sequence_length': seq_length,'sampling_interval': sampling_interval}\n",
    "    with open(str(path/'config.ini'), 'w') as configfile:\n",
    "        config.write(configfile)\n",
    "    \n",
    "    # Load drive logs and paths\n",
    "    #inputs1, targets1 = load_driving_logs(town1_dataset_folders, steering_correction)\n",
    "    #inputs4, targets4 = load_driving_logs(town4_dataset_folders, steering_correction)\n",
    "    inputs, targets = load_driving_logs(dataset_folders, steering_correction)\n",
    "\n",
    "    # Adjust hlcs on town 1 data \n",
    "    if adjust_hlc: \n",
    "        inputs[\"hlcs\"] = inputs(inputs1[\"hlcs\"], inputs[\"info_signals\"])\n",
    "    \n",
    "    # Prepare dataset to model format \n",
    "    #inputs_flat1, targets_flat1 = prepare_dataset_lstm(inputs1, targets1, sampling_interval, seq_length)\n",
    "    #inputs_flat4, targets_flat4 = prepare_dataset_lstm(inputs4, targets4, sampling_interval, seq_length)\n",
    "    inputs_flat, targets_flat = prepare_dataset_lstm(inputs, targets, sampling_interval, seq_length)\n",
    "\n",
    "    num_before1 = len(inputs_flat[\"forward_imgs\"])\n",
    "    # Balance data \n",
    "    #if balance_data:\n",
    "    #    # Plot data before balancing \n",
    "    #    title_before = \"Town 1 - Before Balancing\"\n",
    "    #    plot_data(get_dist(inputs_flat1), title=title_before)\n",
    "\n",
    "        # Balance data \n",
    "    #    inputs_flat1, targets_flat1, dist_bal1 = balance_data_lstm(inputs_flat1, targets_flat1)\n",
    "\n",
    "        # Plot data after balancing \n",
    "    #    title_after = \"Town 1 - After Balancing\"\n",
    "    #    plot_data(dist_bal1, title=title_after)\n",
    "\n",
    "\n",
    "    inputs_flat_dict = create_input_dict() \n",
    "    targets_flat_dict = create_target_dict() \n",
    "\n",
    "    for key in inputs_flat: \n",
    "        #inputs_flat[key] = np.concatenate((inputs_flat1[key],inputs_flat4[key]))\n",
    "        inputs_flat_dict[key] = inputs_flat[key] \n",
    "\n",
    "    for key in targets_flat: \n",
    "        targets_flat_dict[key] = targets_flat[key]\n",
    "                \n",
    "    # Shuffle data \n",
    "    inputs_flat, targets_flat = shuffle_data(inputs_flat, targets_flat)\n",
    "    \n",
    "    # Plot final data set \n",
    "    #plot_data(get_dist(inputs_flat), title=\"Final distribution\")\n",
    "           \n",
    "    \n",
    "    # Split into val and train\n",
    "    split_pos = int(val_split*len(inputs_flat[\"forward_imgs\"]))\n",
    "    inputs_train, inputs_val = split_dict(inputs_flat, split_pos)\n",
    "    targets_train, targets_val = split_dict(targets_flat, split_pos)\n",
    "\n",
    "    # Print training info \n",
    "    train_num = len(inputs_train[\"forward_imgs\"])\n",
    "    val_num = len(inputs_val[\"forward_imgs\"])\n",
    "    print(\"Initiate training loop with the following parameters:\")\n",
    "    print(\"---\")\n",
    "    print(parameters_string)\n",
    "    print(\"---\")\n",
    "    print(\"Training set size: \" + str(train_num))\n",
    "    print(\"Validation set size: \" + str(val_num))\n",
    " \n",
    "    # Get model\n",
    "    model = get_lstm_model(seq_length ,print_summary=False)\n",
    "\n",
    "    \n",
    "    # Compile model \n",
    "    model.compile(loss=[steer_loss(), mean_squared_error, mean_squared_error] , optimizer=Adam())\n",
    "\n",
    "    checkpoint_val = ModelCheckpoint(str(path / ('{epoch:02d}_s{val_steer_pred_loss:.4f}_t{val_throttle_pred_loss:.4f}_b{val_brake_pred_loss:.4f}.h5')), monitor='val_loss', verbose=1, save_best_only=False,mode=\"min\")\n",
    "    \n",
    "    # Create image of model architecture \n",
    "    plot_model(model, str(path/'model.png'))\n",
    "    \n",
    "    steps = int(train_num/batch_size)\n",
    "    steps_val = int(val_num/batch_size)\n",
    "     \n",
    "    # Train model\n",
    "    history_object = model.fit_generator(\n",
    "        generator(inputs_train, targets_train, batch_size),\n",
    "        validation_data=generator(inputs_val, targets_val, batch_size, validation=True),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint_val],\n",
    "        steps_per_epoch=steps,\n",
    "        validation_steps = steps_val,\n",
    "        use_multiprocessing = True,\n",
    "        workers=10\n",
    "    )\n",
    "    \n",
    "    # Prepare plot and save it to disk\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    print(\"History\", history_object.history)\n",
    "    ax.plot(history_object.history['steer_pred_loss'], color=\"blue\")\n",
    "    ax.plot(history_object.history['val_steer_pred_loss'], color=\"blue\", linestyle=\"--\")\n",
    "    ax.plot(history_object.history['throttle_pred_loss'], color=\"green\")\n",
    "    ax.plot(history_object.history['val_throttle_pred_loss'], color=\"green\", linestyle=\"--\")\n",
    "    ax.plot(history_object.history['brake_pred_loss'], color=\"red\")\n",
    "    ax.plot(history_object.history['val_brake_pred_loss'], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_title(\"Mean squared loss of: throttle, brake and steer\")\n",
    "    ax.set_xlabel(\"epochs\")\n",
    "    ax.set_ylabel(\"mse\")\n",
    "\n",
    "    lgd = ax.legend(['steer loss', \n",
    "               'steer validation loss',\n",
    "               'throttle loss', \n",
    "               'throttle validation loss', \n",
    "               'brake loss', \n",
    "               'brake validation loss'], bbox_to_anchor=(1.1, 1.05))\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(str(path / 'loss.png'), bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "    print('\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 28, 129, 137],\n",
       "        [ 16, 130, 133],\n",
       "        [ 15, 130, 133],\n",
       "        ...,\n",
       "        [111, 136,  91],\n",
       "        [103, 137,  91],\n",
       "        [ 95, 138,  90]],\n",
       "\n",
       "       [[ 39, 129, 138],\n",
       "        [ 14, 130, 134],\n",
       "        [ 23, 130, 136],\n",
       "        ...,\n",
       "        [118, 136,  91],\n",
       "        [110, 136,  91],\n",
       "        [102, 137,  91]],\n",
       "\n",
       "       [[ 36, 129, 141],\n",
       "        [ 26, 130, 139],\n",
       "        [ 27, 128, 139],\n",
       "        ...,\n",
       "        [126, 134,  93],\n",
       "        [117, 136,  91],\n",
       "        [109, 136,  91]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 47, 129, 132],\n",
       "        [ 46, 130, 130],\n",
       "        [ 42, 130, 130],\n",
       "        ...,\n",
       "        [ 34, 127, 130],\n",
       "        [ 30, 128, 128],\n",
       "        [ 28, 129, 124]],\n",
       "\n",
       "       [[ 46, 130, 130],\n",
       "        [ 52, 129, 130],\n",
       "        [ 45, 130, 130],\n",
       "        ...,\n",
       "        [ 36, 127, 130],\n",
       "        [ 39, 128, 128],\n",
       "        [ 39, 129, 125]],\n",
       "\n",
       "       [[ 46, 130, 130],\n",
       "        [ 51, 129, 131],\n",
       "        [ 45, 130, 130],\n",
       "        ...,\n",
       "        [ 33, 125, 130],\n",
       "        [ 39, 128, 128],\n",
       "        [ 36, 129, 124]]], dtype=uint8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla",
   "language": "python",
   "name": "carla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
